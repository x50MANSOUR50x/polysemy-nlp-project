{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718a9831",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea7a56",
   "metadata": {},
   "source": [
    "ðŸ“Œ `Natural Language Toolkit (NLTK)` is one of the largest Python libraries for performing various Natural Language Processing tasks.\n",
    " \n",
    "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00933796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading NLTK\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4a4c8",
   "metadata": {},
   "source": [
    "ðŸ“Œ Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline.\n",
    "\n",
    "Let us take a look at the two major types of tokenization provided by NLTK, along with one manual method(character tokenization):."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a345f",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa88505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fd97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10021b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52918c19",
   "metadata": {},
   "source": [
    "### Word Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6b1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc785fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfacd4c",
   "metadata": {},
   "source": [
    "### Character Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bf88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_character = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54789e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in text:\n",
    "    tokenized_character.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b922b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'M', 'r', '.', ' ', 'S', 'm', 'i', 't', 'h', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 't', 'o', 'd', 'a', 'y', '?', ' ', 'T', 'h', 'e', ' ', 'w', 'e', 'a', 't', 'h', 'e', 'r', ' ', 'i', 's', ' ', 'g', 'r', 'e', 'a', 't', ',', ' ', 'a', 'n', 'd', ' ', 'c', 'i', 't', 'y', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '.', '\\n', 'T', 'h', 'e', ' ', 's', 'k', 'y', ' ', 'i', 's', ' ', 'p', 'i', 'n', 'k', 'i', 's', 'h', '-', 'b', 'l', 'u', 'e', '.', ' ', 'Y', 'o', 'u', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'n', \"'\", 't', ' ', 'e', 'a', 't', ' ', 'c', 'a', 'r', 'd', 'b', 'o', 'a', 'r', 'd']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a686c59",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe35bcc",
   "metadata": {},
   "source": [
    "ðŸ“Œ Stop words are common words in a language that are often filtered out during nlp tasks because they carry little meaning or contribute minimally to the overall understanding of a text. Examples include \"the,\" \"is,\" \"and,\" \"in,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e30937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7153e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "stopwords.words(\"english\")\n",
    "print(type(stopwords.words(\"english\")))\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f562733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "{'and', 'needn', 'won', 'couldn', 'here', \"she'll\", 'below', 'up', \"it's\", 'this', \"it'll\", 'some', 't', 'very', 'yourself', 'no', 'or', 'just', 'can', 'on', 'who', 'when', 'their', 'few', 'mustn', \"don't\", \"i'll\", 'didn', \"it'd\", \"isn't\", \"she's\", 'off', 'so', \"wasn't\", 'ain', 'a', 'than', 'was', 'my', 'were', 'what', \"aren't\", 'having', 'isn', 'too', 'we', 'more', 'do', \"he's\", \"we'll\", \"you'd\", 'nor', 'other', 'been', 's', 'same', \"that'll\", 'down', 'in', 'ma', 'should', 'whom', \"i've\", 'wasn', \"mightn't\", 'don', 'any', 've', \"didn't\", 'yourselves', 'your', 'ourselves', \"should've\", 'am', 'as', 'shan', 'such', 'i', 'again', 'himself', 'during', 'own', \"wouldn't\", 'yours', 'you', 'after', 'most', 'at', \"they'll\", 'between', \"doesn't\", 'shouldn', \"i'd\", \"hasn't\", \"you've\", 'hadn', 'itself', 'him', 'that', \"you're\", 'being', 'while', 'myself', 'weren', 'from', 'her', \"he'll\", 'above', 'wouldn', 'then', 'be', \"you'll\", 'ours', 'are', 'each', 'his', 'aren', 'me', 'our', 'm', 'until', \"we'd\", 'which', 'd', 'where', 'into', \"shan't\", 'did', 'hers', 'of', 'these', 'out', \"he'd\", 'y', \"hadn't\", \"won't\", 'to', 'them', 'over', \"haven't\", 'the', 'has', 'doing', \"mustn't\", 'there', 'against', \"couldn't\", 'an', 'will', 'she', \"she'd\", \"we've\", 'because', \"i'm\", 'o', \"they've\", 'he', 'if', 'those', 'had', 'now', \"they'd\", \"shouldn't\", \"needn't\", 're', 'through', \"weren't\", 'themselves', 'herself', 'under', 'mightn', 'not', 'theirs', 'haven', 'they', 'how', 'all', \"they're\", 'll', 'further', 'hasn', 'why', 'for', 'does', 'with', 'only', 'before', 'doesn', 'its', 'have', 'about', 'but', 'by', 'once', 'it', \"we're\", 'both', 'is'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64184e87",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec53c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sent=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b150800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3938f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03616f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "Tokenized Sentence length: 30\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Filterd Sentence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Sentence:\", tokenized_word)\n",
    "print(\"Tokenized Sentence length:\", len(tokenized_word))\n",
    "print(\"Filterd Sentence:\" , filtered_sent)\n",
    "print(\"Filterd Sentence length:\", len(filtered_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3a711",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32290f",
   "metadata": {},
   "source": [
    "ðŸ“Œ Stemming generates the base word from the inflected word by removing the affixes of the word. It has a set of pre-defined rules that govern the dropping of these affixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d700f9d",
   "metadata": {},
   "source": [
    "ðŸ“Œ It must be noted that stemmers might not always result in semantically meaningful base words.  Stemmers are faster and computationally less expensive than lemmatizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5f379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e73bd3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dbae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e58cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded1d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered Sentence:\", filtered_sent)\n",
    "print(\"Stemmed Sentence:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e655374",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eede0d",
   "metadata": {},
   "source": [
    "ðŸ“Œ Lemmatization involves grouping together the inflected forms of the same word. This way, we can reach out to the base form of any word which will be meaningful in nature. The base form here is called the Lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985372d2",
   "metadata": {},
   "source": [
    "ðŸ“Œ Lemmatizers are slower and computationally more expensive than stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a621681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3356a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1018076",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88362bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083dc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: be\n",
      "Stemmed Word: is\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6ee5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_words = []\n",
    "stem_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6facc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tokenized_word:\n",
    "    lemmatize_words.append(lem.lemmatize(word, 'v'))\n",
    "    stem_words.append(stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33b84c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'be',\n",
       " 'you',\n",
       " 'do',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'be',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'be',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'be',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62c0875b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'mr.',\n",
       " 'smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'do',\n",
       " 'today',\n",
       " '?',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'citi',\n",
       " 'is',\n",
       " 'awesom',\n",
       " '.',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blu',\n",
       " '.',\n",
       " 'you',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "966c69d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 'Hello',\n",
       " 'mr.': 'Mr.',\n",
       " 'smith': 'Smith',\n",
       " ',': ',',\n",
       " 'how': 'how',\n",
       " 'are': 'be',\n",
       " 'you': 'You',\n",
       " 'do': 'do',\n",
       " 'today': 'today',\n",
       " '?': '?',\n",
       " 'the': 'The',\n",
       " 'weather': 'weather',\n",
       " 'is': 'be',\n",
       " 'great': 'great',\n",
       " 'and': 'and',\n",
       " 'citi': 'city',\n",
       " 'awesom': 'awesome',\n",
       " '.': '.',\n",
       " 'sky': 'sky',\n",
       " 'pinkish-blu': 'pinkish-blue',\n",
       " 'should': 'should',\n",
       " \"n't\": \"n't\",\n",
       " 'eat': 'eat',\n",
       " 'cardboard': 'cardboard'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {}\n",
    "\n",
    "for index in range(len(tokenized_word)):\n",
    "    dict[stem_words[index]] = lemmatize_words[index]\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f717c",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677cc21",
   "metadata": {},
   "source": [
    "ðŸ“Œ Part of Speech (POS) tagging refers to assigning each word of a sentence to its part of speech. It is significant as it helps to give a better syntactic overview of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e198ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Albert Einstein was born in Ulm, Germany in 1879.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c512cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einstein', 'was', 'born', 'in', 'Ulm', ',', 'Germany', 'in', '1879', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2adf659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Ulm', 'NNP'),\n",
       " (',', ','),\n",
       " ('Germany', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1879', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c4b818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e2514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC</td>\n",
       "      <td>Coordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CD</td>\n",
       "      <td>Cardinal number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EX</td>\n",
       "      <td>Existential there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FW</td>\n",
       "      <td>Foreign word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>Preposition/subordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JJR</td>\n",
       "      <td>Adjective, comparative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JJS</td>\n",
       "      <td>Adjective, superlative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LS</td>\n",
       "      <td>List item marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MD</td>\n",
       "      <td>Modal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NNS</td>\n",
       "      <td>Noun, plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NNP</td>\n",
       "      <td>Proper noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NNPS</td>\n",
       "      <td>Proper noun, plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PDT</td>\n",
       "      <td>Predeterminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>POS</td>\n",
       "      <td>Possessive ending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PRP$</td>\n",
       "      <td>Possessive pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RB</td>\n",
       "      <td>Adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RBR</td>\n",
       "      <td>Adverb, comparative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RBS</td>\n",
       "      <td>Adverb, superlative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RP</td>\n",
       "      <td>Particle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SYM</td>\n",
       "      <td>Symbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>UH</td>\n",
       "      <td>Interjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VB</td>\n",
       "      <td>Verb, base form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VBD</td>\n",
       "      <td>Verb, past tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VBG</td>\n",
       "      <td>Verb, gerund or present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>VBN</td>\n",
       "      <td>Verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VBP</td>\n",
       "      <td>Verb, non-3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>Verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>WDT</td>\n",
       "      <td>Wh-determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>WP</td>\n",
       "      <td>Wh-pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WP$</td>\n",
       "      <td>Possessive wh-pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>WRB</td>\n",
       "      <td>Wh-adverb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tag                                Meaning\n",
       "0     CC               Coordinating conjunction\n",
       "1     CD                        Cardinal number\n",
       "2     DT                             Determiner\n",
       "3     EX                      Existential there\n",
       "4     FW                           Foreign word\n",
       "5     IN  Preposition/subordinating conjunction\n",
       "6     JJ                              Adjective\n",
       "7    JJR                 Adjective, comparative\n",
       "8    JJS                 Adjective, superlative\n",
       "9     LS                       List item marker\n",
       "10    MD                                  Modal\n",
       "11    NN                 Noun, singular or mass\n",
       "12   NNS                           Noun, plural\n",
       "13   NNP                  Proper noun, singular\n",
       "14  NNPS                    Proper noun, plural\n",
       "15   PDT                          Predeterminer\n",
       "16   POS                      Possessive ending\n",
       "17   PRP                       Personal pronoun\n",
       "18  PRP$                     Possessive pronoun\n",
       "19    RB                                 Adverb\n",
       "20   RBR                    Adverb, comparative\n",
       "21   RBS                    Adverb, superlative\n",
       "22    RP                               Particle\n",
       "23   SYM                                 Symbol\n",
       "24    TO                                     to\n",
       "25    UH                           Interjection\n",
       "26    VB                        Verb, base form\n",
       "27   VBD                       Verb, past tense\n",
       "28   VBG     Verb, gerund or present participle\n",
       "29   VBN                  Verb, past participle\n",
       "30   VBP  Verb, non-3rd person singular present\n",
       "31   VBZ      Verb, 3rd person singular present\n",
       "32   WDT                          Wh-determiner\n",
       "33    WP                             Wh-pronoun\n",
       "34   WP$                  Possessive wh-pronoun\n",
       "35   WRB                              Wh-adverb"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_tags = pd.read_csv(r'Data\\POS_tags.csv')\n",
    "POS_tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
