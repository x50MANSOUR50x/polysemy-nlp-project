{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718a9831",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea7a56",
   "metadata": {},
   "source": [
    "ðŸ“Œ `Natural Language Toolkit (NLTK)` is one of the largest Python libraries for performing various Natural Language Processing tasks.\n",
    " \n",
    "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00933796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading NLTK\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4a4c8",
   "metadata": {},
   "source": [
    "ðŸ“Œ Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline.\n",
    "\n",
    "Let us take a look at the two major types of tokenization provided by NLTK, along with one manual method(character tokenization):."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a345f",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa88505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fd97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10021b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52918c19",
   "metadata": {},
   "source": [
    "### Word Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6b1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc785fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfacd4c",
   "metadata": {},
   "source": [
    "### Character Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bf88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_character = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54789e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in text:\n",
    "    tokenized_character.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b922b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'M', 'r', '.', ' ', 'S', 'm', 'i', 't', 'h', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 't', 'o', 'd', 'a', 'y', '?', ' ', 'T', 'h', 'e', ' ', 'w', 'e', 'a', 't', 'h', 'e', 'r', ' ', 'i', 's', ' ', 'g', 'r', 'e', 'a', 't', ',', ' ', 'a', 'n', 'd', ' ', 'c', 'i', 't', 'y', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '.', '\\n', 'T', 'h', 'e', ' ', 's', 'k', 'y', ' ', 'i', 's', ' ', 'p', 'i', 'n', 'k', 'i', 's', 'h', '-', 'b', 'l', 'u', 'e', '.', ' ', 'Y', 'o', 'u', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'n', \"'\", 't', ' ', 'e', 'a', 't', ' ', 'c', 'a', 'r', 'd', 'b', 'o', 'a', 'r', 'd']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a686c59",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe35bcc",
   "metadata": {},
   "source": [
    "ðŸ“Œ Stop words are common words in a language that are often filtered out during nlp tasks because they carry little meaning or contribute minimally to the overall understanding of a text. Examples include \"the,\" \"is,\" \"and,\" \"in,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e30937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7153e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "stopwords.words(\"english\")\n",
    "print(type(stopwords.words(\"english\")))\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f562733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "{'and', 'needn', 'won', 'couldn', 'here', \"she'll\", 'below', 'up', \"it's\", 'this', \"it'll\", 'some', 't', 'very', 'yourself', 'no', 'or', 'just', 'can', 'on', 'who', 'when', 'their', 'few', 'mustn', \"don't\", \"i'll\", 'didn', \"it'd\", \"isn't\", \"she's\", 'off', 'so', \"wasn't\", 'ain', 'a', 'than', 'was', 'my', 'were', 'what', \"aren't\", 'having', 'isn', 'too', 'we', 'more', 'do', \"he's\", \"we'll\", \"you'd\", 'nor', 'other', 'been', 's', 'same', \"that'll\", 'down', 'in', 'ma', 'should', 'whom', \"i've\", 'wasn', \"mightn't\", 'don', 'any', 've', \"didn't\", 'yourselves', 'your', 'ourselves', \"should've\", 'am', 'as', 'shan', 'such', 'i', 'again', 'himself', 'during', 'own', \"wouldn't\", 'yours', 'you', 'after', 'most', 'at', \"they'll\", 'between', \"doesn't\", 'shouldn', \"i'd\", \"hasn't\", \"you've\", 'hadn', 'itself', 'him', 'that', \"you're\", 'being', 'while', 'myself', 'weren', 'from', 'her', \"he'll\", 'above', 'wouldn', 'then', 'be', \"you'll\", 'ours', 'are', 'each', 'his', 'aren', 'me', 'our', 'm', 'until', \"we'd\", 'which', 'd', 'where', 'into', \"shan't\", 'did', 'hers', 'of', 'these', 'out', \"he'd\", 'y', \"hadn't\", \"won't\", 'to', 'them', 'over', \"haven't\", 'the', 'has', 'doing', \"mustn't\", 'there', 'against', \"couldn't\", 'an', 'will', 'she', \"she'd\", \"we've\", 'because', \"i'm\", 'o', \"they've\", 'he', 'if', 'those', 'had', 'now', \"they'd\", \"shouldn't\", \"needn't\", 're', 'through', \"weren't\", 'themselves', 'herself', 'under', 'mightn', 'not', 'theirs', 'haven', 'they', 'how', 'all', \"they're\", 'll', 'further', 'hasn', 'why', 'for', 'does', 'with', 'only', 'before', 'doesn', 'its', 'have', 'about', 'but', 'by', 'once', 'it', \"we're\", 'both', 'is'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64184e87",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec53c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sent=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b150800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3938f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03616f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "Tokenized Sentence length: 30\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Filterd Sentence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Sentence:\", tokenized_word)\n",
    "print(\"Tokenized Sentence length:\", len(tokenized_word))\n",
    "print(\"Filterd Sentence:\" , filtered_sent)\n",
    "print(\"Filterd Sentence length:\", len(filtered_sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
