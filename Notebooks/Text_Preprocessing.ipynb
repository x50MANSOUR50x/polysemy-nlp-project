{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718a9831",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea7a56",
   "metadata": {},
   "source": [
    "ðŸ“Œ `Natural Language Toolkit (NLTK)` is one of the largest Python libraries for performing various Natural Language Processing tasks.\n",
    " \n",
    "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00933796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading NLTK\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4a4c8",
   "metadata": {},
   "source": [
    "ðŸ“Œ Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline.\n",
    "\n",
    "Let us take a look at the two major types of tokenization provided by NLTK, along with one manual method(character tokenization):."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a345f",
   "metadata": {},
   "source": [
    "### Sentence Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa88505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fd97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n",
    "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10021b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52918c19",
   "metadata": {},
   "source": [
    "### Word Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6b1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc785fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfacd4c",
   "metadata": {},
   "source": [
    "### Character Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bf88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_character = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54789e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in text:\n",
    "    tokenized_character.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b922b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'M', 'r', '.', ' ', 'S', 'm', 'i', 't', 'h', ',', ' ', 'h', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 't', 'o', 'd', 'a', 'y', '?', ' ', 'T', 'h', 'e', ' ', 'w', 'e', 'a', 't', 'h', 'e', 'r', ' ', 'i', 's', ' ', 'g', 'r', 'e', 'a', 't', ',', ' ', 'a', 'n', 'd', ' ', 'c', 'i', 't', 'y', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '.', '\\n', 'T', 'h', 'e', ' ', 's', 'k', 'y', ' ', 'i', 's', ' ', 'p', 'i', 'n', 'k', 'i', 's', 'h', '-', 'b', 'l', 'u', 'e', '.', ' ', 'Y', 'o', 'u', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'n', \"'\", 't', ' ', 'e', 'a', 't', ' ', 'c', 'a', 'r', 'd', 'b', 'o', 'a', 'r', 'd']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a686c59",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe35bcc",
   "metadata": {},
   "source": [
    "ðŸ“Œ Stop words are common words in a language that are often filtered out during nlp tasks because they carry little meaning or contribute minimally to the overall understanding of a text. Examples include \"the,\" \"is,\" \"and,\" \"in,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e30937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7153e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "stopwords.words(\"english\")\n",
    "print(type(stopwords.words(\"english\")))\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f562733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "{'and', 'needn', 'won', 'couldn', 'here', \"she'll\", 'below', 'up', \"it's\", 'this', \"it'll\", 'some', 't', 'very', 'yourself', 'no', 'or', 'just', 'can', 'on', 'who', 'when', 'their', 'few', 'mustn', \"don't\", \"i'll\", 'didn', \"it'd\", \"isn't\", \"she's\", 'off', 'so', \"wasn't\", 'ain', 'a', 'than', 'was', 'my', 'were', 'what', \"aren't\", 'having', 'isn', 'too', 'we', 'more', 'do', \"he's\", \"we'll\", \"you'd\", 'nor', 'other', 'been', 's', 'same', \"that'll\", 'down', 'in', 'ma', 'should', 'whom', \"i've\", 'wasn', \"mightn't\", 'don', 'any', 've', \"didn't\", 'yourselves', 'your', 'ourselves', \"should've\", 'am', 'as', 'shan', 'such', 'i', 'again', 'himself', 'during', 'own', \"wouldn't\", 'yours', 'you', 'after', 'most', 'at', \"they'll\", 'between', \"doesn't\", 'shouldn', \"i'd\", \"hasn't\", \"you've\", 'hadn', 'itself', 'him', 'that', \"you're\", 'being', 'while', 'myself', 'weren', 'from', 'her', \"he'll\", 'above', 'wouldn', 'then', 'be', \"you'll\", 'ours', 'are', 'each', 'his', 'aren', 'me', 'our', 'm', 'until', \"we'd\", 'which', 'd', 'where', 'into', \"shan't\", 'did', 'hers', 'of', 'these', 'out', \"he'd\", 'y', \"hadn't\", \"won't\", 'to', 'them', 'over', \"haven't\", 'the', 'has', 'doing', \"mustn't\", 'there', 'against', \"couldn't\", 'an', 'will', 'she', \"she'd\", \"we've\", 'because', \"i'm\", 'o', \"they've\", 'he', 'if', 'those', 'had', 'now', \"they'd\", \"shouldn't\", \"needn't\", 're', 'through', \"weren't\", 'themselves', 'herself', 'under', 'mightn', 'not', 'theirs', 'haven', 'they', 'how', 'all', \"they're\", 'll', 'further', 'hasn', 'why', 'for', 'does', 'with', 'only', 'before', 'doesn', 'its', 'have', 'about', 'but', 'by', 'once', 'it', \"we're\", 'both', 'is'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64184e87",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec53c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sent=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b150800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3938f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03616f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "Tokenized Sentence length: 30\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Filterd Sentence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Sentence:\", tokenized_word)\n",
    "print(\"Tokenized Sentence length:\", len(tokenized_word))\n",
    "print(\"Filterd Sentence:\" , filtered_sent)\n",
    "print(\"Filterd Sentence length:\", len(filtered_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3a711",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32290f",
   "metadata": {},
   "source": [
    "ðŸ“Œ Stemming generates the base word from the inflected word by removing the affixes of the word. It has a set of pre-defined rules that govern the dropping of these affixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d700f9d",
   "metadata": {},
   "source": [
    "ðŸ“Œ It must be noted that stemmers might not always result in semantically meaningful base words.  Stemmers are faster and computationally less expensive than lemmatizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5f379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e73bd3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dbae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e58cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded1d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtered Sentence:\", filtered_sent)\n",
    "print(\"Stemmed Sentence:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e655374",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eede0d",
   "metadata": {},
   "source": [
    "ðŸ“Œ Lemmatization involves grouping together the inflected forms of the same word. This way, we can reach out to the base form of any word which will be meaningful in nature. The base form here is called the Lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985372d2",
   "metadata": {},
   "source": [
    "ðŸ“Œ Lemmatizers are slower and computationally more expensive than stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a621681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3356a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1018076",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88362bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083dc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: be\n",
      "Stemmed Word: is\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6ee5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_words = []\n",
    "stem_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6facc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tokenized_word:\n",
    "    lemmatize_words.append(lem.lemmatize(word, 'v'))\n",
    "    stem_words.append(stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33b84c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'be',\n",
       " 'you',\n",
       " 'do',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'be',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'be',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'be',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62c0875b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'mr.',\n",
       " 'smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'do',\n",
       " 'today',\n",
       " '?',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'citi',\n",
       " 'is',\n",
       " 'awesom',\n",
       " '.',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blu',\n",
       " '.',\n",
       " 'you',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "966c69d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 'Hello',\n",
       " 'mr.': 'Mr.',\n",
       " 'smith': 'Smith',\n",
       " ',': ',',\n",
       " 'how': 'how',\n",
       " 'are': 'be',\n",
       " 'you': 'You',\n",
       " 'do': 'do',\n",
       " 'today': 'today',\n",
       " '?': '?',\n",
       " 'the': 'The',\n",
       " 'weather': 'weather',\n",
       " 'is': 'be',\n",
       " 'great': 'great',\n",
       " 'and': 'and',\n",
       " 'citi': 'city',\n",
       " 'awesom': 'awesome',\n",
       " '.': '.',\n",
       " 'sky': 'sky',\n",
       " 'pinkish-blu': 'pinkish-blue',\n",
       " 'should': 'should',\n",
       " \"n't\": \"n't\",\n",
       " 'eat': 'eat',\n",
       " 'cardboard': 'cardboard'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {}\n",
    "\n",
    "for index in range(len(tokenized_word)):\n",
    "    dict[stem_words[index]] = lemmatize_words[index]\n",
    "\n",
    "dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
